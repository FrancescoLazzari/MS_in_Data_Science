---
title: "Statistical Learning \\ HW1 P2"
author: "G06 - Francesco Lazzari, Camilla Brìgandi, Riccardo Violano, Matteo Pazzini,
  Paolo Meli"
date: "Academic year 2023/2024"
output: 
  rmarkdown::html_document:
    theme: flatly
    toc: yes
    toc_float:
      collapsed: true
editor_options: 
  markdown: 
    wrap: 72 
---

```{=html}
<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
body {
  text-align: justify;
}
p.caption {
  font-size: 0.6em;
}
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Libraries etc, echo=FALSE, warning=F, error=F, message=F}
# Cleaning the workspace -----------------------------
rm(list = ls())

# Libraries ------------------------------------------
if (!requireNamespace("ggplot2", quietly = TRUE))
    install.packages("ggplot2")

if (!requireNamespace("ggmap", quietly = TRUE))
    install.packages("ggmap")

if (!requireNamespace("labstatR", quietly = TRUE))
    install.packages("labstatR")

if (!requireNamespace("DescTools", quietly = TRUE))
    install.packages("DescTools")

if (!requireNamespace("plotly", quietly = TRUE))
    install.packages("plotly")

if (!requireNamespace("stats", quietly = TRUE))
    install.packages("stats")

if (!requireNamespace("gridExtra", quietly = TRUE))
    install.packages("gridExtra")

if (!requireNamespace("ggpointdensity", quietly = TRUE))
    install.packages("ggpointdensity")

if (!requireNamespace("whitening", quietly = TRUE))
    install.packages("whitening")

if (!requireNamespace("ks", quietly = TRUE))
    install.packages("ks")

if (!requireNamespace("MASSExtra", quietly = TRUE))
    install.packages("MASSExtra")

if (!requireNamespace("meanShiftR", quietly = TRUE))
    install.packages("meanShiftR")

if (!requireNamespace("dbscan", quietly = TRUE))
    install.packages("dbscan")

if (!requireNamespace("RDP", quietly = TRUE))
    install.packages("RDP")

if (!requireNamespace("SimilarityMeasures", quietly = TRUE))
    install.packages("SimilarityMeasures")

if (!requireNamespace("dplyr", quietly = TRUE))
    install.packages("dplyr")

if (!requireNamespace("ggraph", quietly = TRUE))
    install.packages("ggraph")

if (!requireNamespace("ggdendro", quietly = TRUE))
    install.packages("ggdendro")

if (!requireNamespace("clValid", quietly = TRUE))
    install.packages("clValid")

if (!requireNamespace("patchwork", quietly = TRUE))
    install.packages("patchwork")


library(ggplot2)
library(plotly)
library(stats)
library(gridExtra)
library(ggmap)
library(ggpointdensity)
library(whitening)
library(ks)
library(MASSExtra)
library(meanShiftR)
library(dbscan)
library(RDP)
library(SimilarityMeasures)
library(dplyr) 
library(ggraph)
library(ggdendro)
library(clValid)
library(patchwork)

# Seed for reproducibility ----------------------------
set.seed(12052024)

# Load the Data ---------------------------------------

load("trackme.RData")

# Set the API for the Maps ----------------------------
register_stadiamaps(key = "cf63fa0b-c96f-4131-a58e-caba78807030")
register_google(key = "AIzaSyD5l1Ah1CfHhpv_e8nclbclBA-wBCpB3As")

# Map boundaries
myLocation <- c(min(runtrack$lon, na.rm = T), 
                min(runtrack$lat, na.rm = T), 
                max(runtrack$lon, na.rm = T), 
                max(runtrack$lat, na.rm = T))

# Map download
myMapInD <- get_map(location = myLocation, 
                    source = "stadia", 
                    maptype = "stamen_terrain_lines", 
                    zoom = 15,
                    crop = FALSE)
```
## Point 1 - 2D Kernel Density Estimation

The first thing we are required to do it's to estimate and visualize the density of the *runtrack* by considering only its observations as points in the 2-dimensional euclidean space, using the lon-lat coordinates of each observation. 

Since the choice of the kernel for this application isn't much relevant, we opted for a Gaussian kernel. The important part of this task, on the other hand, is the choice of the optimal bandwidth matrix. We can get around the problem of selecting a full matrix of bandiwith, by whithening the data and using a multiple of the identity matrix as bandwith matrix. This way we can fine-tune the only parameter we need by, for example, minimizing the Leave One Out Cross Validation (LOOCV, from now on) risk. In particoular, to speed up computation by avoiding the estimation of the density excluding on point at a time (which corrseponds to "train" a model each time), for this setting, we can leverage the closed-form formula for the LOOCV risk: 
$$
\widehat{R}_{\mathrm{LOO}}(h)=\frac{\phi^d(0 \mid h \sqrt{2})}{(n-1)}+\frac{n-2}{n(n-1)^2} \sum_{i \neq j} \prod_{\ell=1}^d \phi\left(X_{i, \ell}-X_{j, \ell} \mid h \sqrt{2}\right)-\frac{2}{n(n-1)} \sum_{i \neq j} \prod_{\ell=1}^d \phi\left(X_{i, \ell}-X_{j, \ell} \mid h\right) .
$$
where $\phi(z| \sigma)$ denotes a Normal density with mean 0 and variance $\sigma^2$.

Going to the code, we started by whitening the data using the package *whitening*, and using the Mahalanobis whitening which ensures that the average covariance between whitened and orginal variables is maximal. 

```{r Whitening, class.source = "fold-show"}

# Whitening of the track matrix ------------------------------------------------

df <- data.frame(cbind(runtrack$lon, runtrack$lat))
colnames(df) <-  c("lon", "lat")

# The 'whiten' function require as argument a matrix object
df <- as.matrix(df)

# Whitening
df_whiten <- whiten(df, center = TRUE, method = "ZCA")
colnames(df_whiten) <-  c("lon", "lat")

```



In order to fine-tune the parameter, we first used a wider grid of values for the bandwith, then we did another round of tuning with a smaller and thicker grid of values between $0$ and $0.1$, where the latter is the point that minimized the LOOCV Risk between the values in the first grid. The final bandwith resulting from this procedure is $h= 0.004$, as can be seen from the following plot. Since the computation of the Risk for the first grid took a long time, we saved the results to be able to re-use them without running the tuning again.


```{r Bandwidth tuning,  eval=FALSE}

# Leave One Out Cross Validation Function --------------------------------------


LOOCV_Risk_NormalKernel  <- function(h, d = 2, data = df_whiten){
  
  # Local parameters
  cat(h )                  # see progress
  n <- dim(data)[1]
  sd_par = sqrt(2) * h
  
  # Function for the partial sums
  partial_sums <- function(i){ 
    
    # Calculate the differences in longitude and latitude with a i-th lag
    diff_lon <- diff(data[ , 1], lag = i)
    diff_lat <- diff(data[ , 2], lag = i)
    
    # Calculation of the product of two normal distributions for the second term
    prod_vec1 <- 2 * dnorm(diff_lon, mean = 0, sd = sd_par) * dnorm(diff_lat, mean=0, sd = sd_par)
    partial_sum1 <- sum(prod_vec1)
    
    # Calculation of the product of two normal distributions for the third term
    prod_vec2 <- 2 * dnorm(diff_lon, mean = 0, sd = h) * dnorm(diff_lat, mean = 0, sd = h) 
    partial_sum2 <- sum(prod_vec2)
    
    # Return of the partial sums
    return( c(partial_sum1, partial_sum2) )
  }
  
  # Apply of the 'partial_sums' function to each lag value and store the results in a matrix
  partial_sum_vectors <- apply(X = matrix(1 : n), MARGIN = 1, FUN = partial_sums)
  
  # Calculation of the 3 terms of the LOOCV 
  first_term  <- dnorm(0, mean = 0, sd = sd_par)^d / (n - 1)
  second_term <- ( (n - 2) / (n * (n - 1)^2) ) * sum(partial_sum_vectors[1, ])
  third_term  <- ( 2 / (n * (n - 1)) ) * sum(partial_sum_vectors[2, ])
  
  # Return the estimation of the Risk for the given bandwidth
  return(first_term + second_term - third_term)
}
# Bandwidth parameter tuning, first round --------------------------------------

# Extraction of a grid of possible bandwidth values 
bandwidth_vec <- seq(from = 0, to = 1, length.out = 101)
# We exclude the edge values
bandwidth_vec <- bandwidth_vec[2:100] 

# Extraction of the Risk for each bandwidth of the grid
risk_bandwidth <- apply(X = matrix(bandwidth_vec), MARGIN = 1, FUN = LOOCV_Risk_NormalKernel)

# We save the bandwidth grid and it's associated risk values so that we don't need to riexecute the code 
bw_risk_finegrid <- cbind(bandwidth_vec, risk_bandwidth)
colnames(bw_risk_finegrid) <- c("bandwidth", "LOOCV_risk")

# "Second round" of tuning: restricted grid between 0 and 0.1-------------------

# Construct the grid
bandwidth_vec1 <- seq(from = 0, to = 0.01, length.out = 11)

# Exclude the bound values
bandwidth_vec1 <- bandwidth_vec1[2:10] 

# Get the risk
risk_bandwidth1 <- apply(X=matrix(bandwidth_vec1), MARGIN = 1, FUN = LOOCV_Risk_NormalKernel)

# Save the results
bw_risk_finegrid <- cbind(bandwidth_vec1, risk_bandwidth1)
colnames(bw_risk_finegrid) <- c("bandwidth", "LOOCV_risk")
save(bw_risk_finegrid, file="LOOCV_risk_finegrid.Rdata")

```

```{r Optimal Bandwidth}
# Optimal bandwidth selection --------------------------------------------------

# Load the data
load("LOOCV_risk_finegrid.Rdata")

# Bandwidth which minimize the risk
bw_whiten <- bw_risk_finegrid[ , "bandwidth"][ which.min(bw_risk_finegrid[ , "LOOCV_risk"]) ]
```


```{r Bandwidth tuning plot, echo=FALSE,  warning=FALSE, fig.width=10, fig.height=7}

ggplot(data = bw_risk_finegrid, aes(x = bandwidth, y = LOOCV_risk)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = bw_whiten, linetype = 'dashed', col = '#CD0000') +
  labs(x = 'Bandwidth', y = "LOOCV - Risk", title = "LOOCV Risk Analysis") +
  geom_label(aes(x = 0.0043, y = -1.375, label = "h*"), 
            color = "black", 
            size = 5) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold"),
        panel.grid.minor.x = element_blank() ) + 
  scale_x_continuous( breaks = bw_risk_finegrid[,1])
```

In the end, we can visualize the density estimated on the whiten data using the fine-tuned parameter $h=0.004$ with a contour plot over the scatterplot of the whitened data:
```{r KDE}
# Kernel Density Estimation on whiten data -------------------------------------
kde_whiten <- kde(df_whiten, H = diag(0.004, 2), binned=F, density = T)

```

```{r KDE whiten contour plot, echo=FALSE, fig.width=10, fig.height=7}

grid_whiten <- expand.grid(kde_whiten$eval.points[[1]], kde_whiten$eval.points[[2]] )
grid_whiten$z <- matrix(kde_whiten$estimate, ncol=1)

ggplot() +
  geom_point(data = data.frame(df_whiten), 
             aes(x=lon, y=lat), 
             cex = .2, col = "gray") +
  geom_contour(aes(x = Var1, 
                   y = Var2, 
                   z = z, 
                   colour = after_stat(level)), 
               data = grid_whiten) +
  scale_colour_viridis_c() +
  labs(x = "Longitude", 
       y = "Latitude", 
       title = "Contour plot of the Kernel density estimation on whiten data") +
  theme_void() +
  theme(plot.title = element_text(face = "bold"),
        plot.title.position = 'plot',
        legend.position = 'bottom',
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(0.5, 'cm')) 

```

Another possible solution to estimate the density on the original data is to use an already implemented function to get the bandwidth matrix, and then estimate the density directly on the original data using this matrix. This approach leads us to have the following heatmap of the density of the datapoints:

```{r KDE original data}
# Kernel Density Estimation on the original data -------------------------------

bw <-  Hpi(df)  # df id the matrix of the long-lat data points
kde_density <- kde(df, eval.points = df, H = bw, binned = F, density = T)

# KDE matrix
dens <- kde_density$eval.points
dens <- cbind(dens, as.matrix(kde_density$estimate))
colnames(dens) <-  c("lon", "lat", 'Density')
dens[,3] <- dens[,3] / max(dens[,3])
```

```{r KDE plot, echo=FALSE, fig.width=10, fig.height=7}

my_rainbow <- colorRampPalette(c("darkgreen", "royalblue", "mediumorchid", "pink1", "#CD0000", "orange1", "gold"))

ggmap(myMapInD) +
  geom_point(data = dens, aes(x = lon, y = lat, color = Density), size = 0.2, alpha = 0.2) +
  scale_color_gradientn(colours = my_rainbow(7)) +
  theme_void() +
  theme(legend.position = "bottom", 
        legend.key.width = unit(2, "cm"),
        plot.title = element_text(face = "bold")) +
  labs(title = 'KDE Heatmap of the running path')

```

### 3D plot 

A third option is to estimate the KDE on a fine grid and plot the 3D version of the previous two plot. We chose to do this on the whitened data using the optimal bandwidth that we found previously with the LOOCV. 

```{r 3D KDE plot, echo=FALSE, eval=TRUE, fig.width=10, fig.height=7}

df_whiten <- data.frame(df_whiten)

dens_grid <- kde2d(df_whiten$lon, df_whiten$lat, n = 1000, lims = c(range(df_whiten$lon), range(df_whiten$lat)), h = bw_whiten)

plot_ly(z = dens_grid$z / max(dens_grid$z)) %>%
  add_surface(x = dens_grid$x, y = dens_grid$y) %>%
  layout(scene = list(
    xaxis = list(title = "Longitude"),
    yaxis = list(title = "Latitude"),
    zaxis = list(title = "Density")
  ))

```

## Point 2 - Clustering


### MeanShift

As we saw in class, mean shift is an algorithm of gradient ascent that should converge to the modes of the distribution. Here we used the estimated kde on the whitened data to find the modes of the distribution.


```{r Mean Shift, eval=FALSE}
# Mean Shift -------------------------------------------------------------------

ms <- meanShift(queryData = as.matrix(df_whiten), bandwidth = rep(bw_whiten,2), epsilonCluster = 1)

# Save the results
save(ms, file = "Mean_Shift_output.Rdata")

```

```{r Load MS, echo=FALSE}
# load the Mean Shift output
load("/Users/francescolazzari/Downloads/Mean_Shift_output.Rdata")
```

The following plot highlights the modes found by the algorithm.


```{r MS centrids}
# MS Centroids extraction ------------------------------------------------------

ms_centroids <- cbind(as.matrix(unique(ms$value)), seq(1, dim(unique(ms$value))[1], by = 1))

```



```{r Mean Shift Centroids plot, echo=FALSE, fig.width=10, fig.height=7}
ggplot() +
  geom_point(data = data.frame(df_whiten), aes(x=lon, y=lat), cex = .2, col = "gray") +
  geom_contour(aes(x = Var1, 
                   y = Var2, 
                   z = z, 
                   colour = after_stat(level)), 
               data = grid_whiten) +
  scale_colour_viridis_c() +
  geom_point(data = data.frame(ms_centroids), 
             aes(x = X1, y = X2), 
             shape = 16, 
             size = 2.5, 
             stroke = 1, 
             color = "#CD0000") +
  labs(x = "Longitude", 
       y = "Latitude", 
       title = "Contour Plot with Mean Shift centroids") +
  theme_void() +
  theme(plot.title = element_text(face = "bold"),
        plot.title.position = 'plot',
                legend.position = 'bottom',
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(0.5, 'cm')) 
```

As we can see from the plot, the algorithm seems to fail since the modes found don't represent the true modes of the distribution.

In the following plot, instead, we can see the assignment of the data points to each cluster.


```{r Mean Shift cluster plot, echo=FALSE, fig.width=10, fig.height=7}
# MS Cluster plot --------------------------------------------------------------

ms_data <- cbind(df, ms$assignment)
colnames(ms_data) <- c("lon", "lat", 'cluster')

ggmap(myMapInD) + 
  geom_point(data = ms_data, 
             aes(x = lon, 
                 y = lat, 
                 color = factor(cluster)), 
             size = 0.2) +  
  labs(title = "Run tracks by Mean Shift clusters") +
  theme_void() +
  theme(plot.title = element_text(face = "bold"),
        plot.title.position = 'plot',
        legend.position = 'bottom',
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(1, 'cm')) +
  guides(color = guide_legend(title = 'Cluster:', override.aes = list(size = 5)))
```



### Optics DBSCAN

DBSCAN is a density-based clustering algorithm that operates by grouping together datapoints density-reachable from each other. Firstly the algorithm estimates the density around each point $(p)$ using the concept of $N_{\epsilon}(p)$ neighborhood which is set of point within a specified radius $\epsilon$, given some distance measure $d$.

$$ N_{\epsilon}(p) = \{ q | d(p,q) \le \epsilon \} $$
Following this definition the size of the neighborhood $|N_{\epsilon}(p) |$ can be seen as a un-normalized kernel density estimate around $p$ using a uniform kernel and a bandwidth of $\epsilon$. 


After estimating the density around each point $p$ using the $N_{\epsilon}(p)$ neighborhood, DBSCAN proceeds with the following steps:


- **Core Point Identification:** A point $p$ is identified as a core point if $|N_{\epsilon}(p)|$, is greater than or equal to the specified parameter $minPts$ which is the minimum size (of points) that a cluster should have. 
$$|N_{\epsilon}(p)| \geq minPts \quad \Rightarrow \quad p \text{ is classified as a core point.}$$
  
- **Density-Reachability:** A point $p$ is considered density-reachable from another point $q$ if $q$ is a core point and $p$ is within the $\epsilon$-neighborhood of $q$. 
  
- **Cluster Formation:** Starting with an arbitrary core point, DBSCAN iteratively expands the cluster by including all density-reachable points. Points that are not density-reachable from any core point are labeled as noise points and do not belong to any cluster.
  
- **Border Point Assignment:** Points that are within the $\epsilon$-neighborhood of a core point but do not meet the criteria to be core points themselves are classified as border points.
  
OPTICS is an extension of DBSCAN that provides a hierarchical clustering of the dataset based on density reachability. It computes a reachability distance for each point in the dataset, which measures the distance at which a point can be reached while following the densest path.

The key advantage of OPTICS over DBSCAN is its ability to identify clusters of varying densities and to provide a hierarchical representation of the data. Additionally, OPTICS facilitates the selection of the $\epsilon$ parameter by inspecting the Reachability plot.

We fixed that each cluster should at least have 1000 data points (arbitrary choice). We chose this value since this $minPts$ value is approx the 2% of the number of data points (~55K) so, since we are searching clusters that are centered on the area with highest density we know that such area should at least have this number of data points.

```{r Optics Structure, eval=FALSE}

# Extraction of the Optics structure -------------------------------------------

optics_structure <- optics(df_whiten, eps = NULL, minPts = 1000)  

# Save the results
save(optics_structure, file = "Optics_structure.Rdata")

```

```{r Load Optics structure, echo=FALSE}
# load the Optics structure
load("/Users/francescolazzari/Downloads/Optics_structure.Rdata")
```

Once the hierarchical structure of the OPTICS DBSCAN is identified, we plot the Reachability plot. After the inspection, we notice the presence of many peaks (noise points), while potential clusters are represented by valleys in the graph. 

Since we are aiming to identify clusters centered around the area with high density of the distribution (which, from the inspection of the kernel density estimation, we know to be approximately 4/5), we choose a low value of epsilon to reduce the number of clusters (everything above the epsilon threshold will be considered as noise).

```{r Reachability plot, echo=FALSE, warning=FALSE, fig.width=10, fig.height=7}

# Extract the order of the data points for the Reachability plot
order <- optics_structure$order

# Extract the Reachability distance for each point (in the original order)
reachability <- optics_structure$reachdist

# Initialization of a new matrix for the Reachability distances in the right order
reach_matrix <- matrix(NA, length(order), 2)
colnames(reach_matrix) <-  c('Index', 'Reachability')
reach_matrix[,1] <- seq(1:length(order))

# Construction of the Reachability plot matrix 
j <- 0
for (i in order ) {
  reach_matrix[j,2] <- reachability[i]
  j <- j + 1
}

# Reachability plot
ggplot(reach_matrix, aes(x = Index, y = Reachability )) +
  geom_bar(stat = "identity", fill = "royalblue") +
  geom_hline(yintercept = 0.1, linetype = 'dashed', col = '#CD0000') +
  labs(title = "Reachability Plot", y = 'Reachability distance') +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold"),
        plot.title.position = 'plot') +
  scale_x_continuous( breaks = seq(from = 0, to = 50000, by = 10000))
```

 After some tuning, we found an optimal value equal of $\epsilon^* = 0.1$. So given this value we retrieve the associated DBSCAN cluster. 
 
 
```{r DBSCAN cluster}
# Extraction of a DBSCAN cluster -----------------------------------------------

optics_results <- extractDBSCAN(optics_structure, eps_cl = 0.1)

optics_centroids <- aggregate(df_whiten[,c('lon','lat')], by = list(cluster_assignments = optics_results$cluster), FUN = mean)

```
 

```{r Optics centroids plot, echo=FALSE, fig.width=10, fig.height=7}
ggplot() +
  geom_point(data = data.frame(df_whiten), aes(x=lon, y=lat), cex = .2, col = "gray") +
  geom_contour(aes(x = Var1, 
                   y = Var2, 
                   z = z, 
                   colour = after_stat(level)), 
               data = grid_whiten) +
  scale_colour_viridis_c() +
  geom_point(data = data.frame(optics_centroids), 
             aes(x = lon, y = lat), 
             shape = 16, 
             size = 2.5, 
             stroke = 1, 
             color = "#CD0000") +
  labs(x = "Longitude", 
       y = "Latitude", 
       title = "Contour Plot with Optics-DBSCAN centroids") +
  theme_void() +
  theme(plot.title = element_text(face = "bold"),
        plot.title.position = 'plot',
        legend.position = 'bottom',
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(0.5, 'cm')) 
```
As expected, the DBSCAN perfectly finds cluster centered on the high density areas identifying the 5 most frequented areas. 
One might think that the 6th centroid is slightly off-center compared to the nearby 'high-density' area. However, that centroid represents the center of cluster 0, which groups all points considered as noise by the algorithm. Therefore, that centroid should not be interpreted like the previous ones. 

In light of all this, we are very pleased with the result and believe we have outperformed the mean shift algorithm in identifying the most frequented areas during running sessions.

Now let's plot the runtracks point after the Optics-DBSCAN clustering.

```{r Optics cluster plot, echo=FALSE, fig.width=10, fig.height=7}
# Optics DBSCAN Cluster plot ---------------------------------------------------

optics_data <- cbind(df, optics_results$cluster)
colnames(optics_data) <- c("lon", "lat", 'cluster')

ggmap(myMapInD) + 
  geom_point(data = optics_data, 
             aes(x = lon, 
                 y = lat, 
                 color = factor(cluster)), 
             size = 0.2) +  
  labs(title = "Run tracks by Optics - DBSCAN clusters") +
  theme_void() +
  theme(plot.title = element_text(face = "bold"),
        plot.title.position = 'plot',
        legend.position = 'bottom',
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(1, 'cm')) +
  guides(color = guide_legend(title = 'Cluster:', override.aes = list(size = 5)))
```

## Point 3 - Functional Data Analysis

At this point, we were asked to actually consider the data not as a bunch of point, but the realization of some curves (i.e. the runtracks) in the plane, and then to identify the 5 paths that have more chance to be runned. 

In order to solve this task we choose to apply a hierarchical clustering procedure on the runtracks using the Frechet distance between them, and then select the clusters with more curves in it to get the most traveled paths. 

First of all, we grouped the rows of the *runtrack* dataframe and created a list of runs contianing the data we need in order to access them easily.

```{r Path data}
# Group data by run
by_id <- runtrack %>% group_by(id)

# See rows in each group
group_rows <- by_id %>% group_rows() 

# Initialize a list of observed data
run_list <- list()

 # Number of runs
n = length(group_rows)

for(i in 1:n){
  
  # Select the rows of the current run from the group_by list
  selected_rows = group_rows[[i]]
  
  # Create data frame of the current run
  current_run <- runtrack[selected_rows, ]
  
  # Create the array of lon-lat data and append it to the list
  LL_data <- matrix( c(current_run$lon, current_run$lat), ncol=2 ) 
  
  run_list[[i]] <- LL_data
}
```

The second step was to apply the Douglas-Peucker algorithm in order to have a smaller number of point per run without modifying its geometry to speed up the computation of the matrix of Frechet distance between the runs, which is needed for the clustering operation. 

The algorithm is an iterative one and needs the choice of a parameter $\epsilon$.  Given the input, it starts from selecting the first and the last point, "virtually" traces the line that connects these points and then considers the furthest one from this line:

- if the distance between this objects is less then $\epsilon$, the point is removed and the algorithm restarts until there are no more points that can be discarded; 

- if the distance is bigger than $\epsilon$, the point is not discarded, but the algorithm splits in two part considers the further point in question as a new ending point of the curve and repeating the procedure starting from a line that connects this point and the first one, and this point and the last one.

The parameter $\epsilon$ is here setted at $1e-4$, since by looking at the resulting curves we saw that they were still approximated in a good way by the remaining set of points (in particular, the points in the "corners" of the run where perserved, while the points discarded were the ones lying on an approximately straight street).


```{r RDP algorithm}
# Initialize list of ligther version of run\curves 
light_run = list()

for(i in 1:n){
  # Select i-th run
  current_run <-  run_list[[i]]
  
  # Apply RDP algorithm on the run and save the result in the list
  light_run[[i]] <- as.matrix(
    RamerDouglasPeucker(current_run[,1], current_run[,2], 1e-4 )
    )
}
```

To give an example, these are the points representing the first run before and after the application of the 
 Douglas-Peucker algorithm.
 
```{r point red plot, echo=FALSE, fig.width=10, fig.height=7}

ggmap(myMapInD) +
  geom_point(data = data.frame(run_list[[1]]), 
             aes(x = X1, y = X2), 
             col = "#b3b3ff", 
             size = 0.2) + 
  geom_point(data = data.frame(light_run[[1]]), 
             aes(x = x, y = y), 
             col = "darkblue", 
             size = 0.2) +
  theme_void() +
  theme(legend.position = "bottom", 
        legend.key.width = unit(2, "cm"),
        plot.title = element_text(face = "bold")) +
  labs(title = 'Douglas-Peucker algorithm on run_1')

```

We computed the Frechet matrix of distances using the *Frechet* function of the *SimilarityMetrics* package, and we saved the results since the computation wasn't fast, then we transformed the matrix in a *dist* object as needed from the *hclust* function and then computed the cluster object using the latter function.

```{r Frechet distance matrix, eval=FALSE}
# Distances Matrix
light_dist_matrix <- matrix(data = NA, nrow = n, ncol = n )

for(i in 2:n){
  cat(i)
  # Select i-th run matrix
  ith_run <- light_run[[i]]
  for(j in 1:(i-1)){
    # Select j-th run matrix
    jth_run <- light_run[[j]]
    
    # Compute Frechet distance between runs
    current_frechet <- Frechet(ith_run, jth_run)
    
    # Allocate values in the matrix
    light_dist_matrix[i, j] = current_frechet
    light_dist_matrix[j, i] = current_frechet
    
  }
}

# Save the results
save(light_dist_matrix, file = "Light_Frechet_Matrix.Rdata")

```


```{r Hierarchical clustering}
# Load the data
load("Light_Frechet_Matrix.Rdata")

# Set 0 as diagonal values of the matrix
diag(light_dist_matrix) <- 0

# Convert matrix to a "dist" object 
dd <- as.dist(light_dist_matrix, diag=T)

# Apply hierarchical clustering
clust_obj <- hclust(dd)

```

The dendrogram resulting from this procedure is the following:


```{r Dendrogram, echo=FALSE, fig.width=10, fig.height=7}
ggdendrogram(clust_obj) + 
  theme_minimal() +
  labs(title = "Dendrogram of Hierarchical clustering of run path ", 
       y = 'Fusion level',
       x = 'Path ID') +
  theme(plot.title = element_text(face = "bold"),
        plot.title.position = 'plot')

```

Now that we have a cluster object we can compute the Dunn Index varying the number of clusters to investigate what number of clusters best represents our dataset.

```{r Dunn Index}
minc <- 5 # minimum number of clusters
maxc <- 20 # maximum number of clusters

# Initialize vector for the Dunn Index
dunn_scores = rep(NA, maxc-minc)

for(k in minc:maxc){
  # Extract vector of clusters' label membership
  ct <- cutree(clust_obj, k = k)
  
  # Compute Dunn Index for this divisiton in clusters and save the resutlt in the vetor
  dunn <- dunn(distance = light_dist_matrix, clusters = ct)
  dunn_scores[(k - minc +1)] <- dunn
}
```


```{r Dunn Index plot, echo=FALSE,  warning=FALSE, fig.width=10, fig.height=7}

dunn_df <- data.frame(minc:maxc, dunn_scores)
colnames(dunn_df) <- c("nclus", "score")

ggplot(data = dunn_df, aes(x = nclus, y = score)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = 10, 
             linetype = 'dashed', 
             col = '#CD0000') +
  labs(x = 'Number of clusters', 
       y = "Dunn Index", 
       title = "Dunn Index for nuber of clusters") +
  #geom_label(aes(x = 9.2, y = 0.25, label = "k=10"),   color = "black", size = 3) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold"),
        panel.grid.minor.x = element_blank() ) + 
  scale_x_continuous( breaks =dunn_df$nclus)
```

As can be seen from the previous plot, the number of clusters that mazimizes the Dunn Index, and thus better represents our data is 10. The resulting clusters can be highlighted in the dendrogram as follows: 

```{r Cluster sep, echo=FALSE,  warning=FALSE}
plot(clust_obj)
rect.hclust(clust_obj, k = 10, border = 2:31)
```


Now we have to see which are the 5 clusters with the higher number of runs assigned to identify the top-5 (groups of) runs. From the following table, it's clear that the top-5 paths are the ones represented in clusters $6$, $1$, $8$, $9$, $5$.


```{r Top 5 paths}
# Extract assignments of the runs to the clusters
clusk10 <- cutree(clust_obj, k = 10)
dfclusk10 <- data.frame(assignments = clusk10)

# Extract number of runs per cluster (they are ordered by the cluster label)
nrun_per_clus <- data.frame(dfclusk10 %>% group_by(assignments) %>% tally()) 
nrun_per_clus[order(nrun_per_clus$n, decreasing = TRUE),]
```

We can finally represent the paths by selecting some runs for each cluster and plot them together on the map.





```{r Cluster runtrack plot, fig.cap="From left to right and from top to bottom, we can see 3 runs of, respectively, cluster number 6, 1, 8, 9, 5" , fig.width=10, fig.height=7, warning=FALSE, echo=FALSE}

clus6 <- which(clusk10 == 6)

pclus6 <- ggmap(myMapInD) + 
  geom_path(data = data.frame(run_list[[ clus6[1] ]], id = clus6[1]), 
            aes(x = X1, y = X2), 
            col = "#0b5394", 
            size = 1.5, 
            lineend = "round",
            alpha = .6) +
  geom_path(data = data.frame(run_list[[ clus6[2] ]], id = clus6[2]), 
            aes(x = X1, y = X2), 
            col = "#3d85c6", 
            size = 1.5, 
            lineend = "round", 
            alpha = .6) +
  geom_path(data = data.frame(run_list[[ clus6[3] ]], id = clus6[5]), 
            aes(x = X1, y = X2), 
            col = "#6fa8dc",
            size = 1.5, 
            lineend = "round", 
            alpha = .6)



clus1 <-  which(clusk10 == 1)

pclus1 <- ggmap(myMapInD) + 
  geom_path(data = data.frame(run_list[[ clus1[1] ]], id = clus1[3]), 
            aes(x = X1, y = X2) , 
            col = "#669900", 
            size = 1.5, 
            lineend = "round", 
            alpha = .6) +
  geom_path(data = data.frame(run_list[[ clus1[2] ]], id = clus1[7]), 
            aes(x = X1, y = X2), 
            col = "#8fce00", 
            size = 1.5, 
            lineend = "round", 
            alpha = .6) +
  geom_path(data = data.frame(run_list[[ clus1[3] ]], id = clus1[10]), 
            aes(x = X1, y = X2), 
            col = "#b0bf1a", 
            size = 1.5, 
            lineend = "round", 
            alpha = .6) 

clus8 <-  which(clusk10 == 8)

pclus8 <- ggmap(myMapInD) + 
  geom_path(data = data.frame(run_list[[ clus8[1] ]], id = clus8[3]), 
            aes(x = X1, y = X2), 
            col = "#9966cc", 
            size = 1.5, 
            lineend = "round", 
            alpha = .6) +
  geom_path(data = data.frame(run_list[[ clus8[2] ]], id = clus8[7]), 
            aes(x = X1, y = X2), 
            col = "#d19fe8", 
            size = 1.5, 
            lineend = "round", 
            alpha = .6) +
  geom_path(data = data.frame(run_list[[ clus8[3] ]], id = clus8[1]), 
            aes(x = X1, y = X2), 
            col = "#cab2f0", 
            size = 1.5, 
            lineend = "round", 
            alpha = .6)

clus9 <-  which(clusk10 == 9)

pclus9 <- ggmap(myMapInD) + 
  geom_path(data = data.frame(run_list[[ clus9[1] ]], id = clus9[1]), 
            aes(x = X1, y = X2), 
            col = "#ff6600", 
            size = 1.5, 
            lineend = "round", 
            alpha = .6) +
  geom_path(data = data.frame(run_list[[ clus9[2] ]], id = clus9[2]), 
            aes(x = X1, y = X2), 
            col = "#ff9933", 
            size = 1.5, 
            lineend = "round", 
            alpha = .6) +
  geom_path(data = data.frame(run_list[[ clus9[3] ]], id = clus9[3]), 
            aes(x = X1, y = X2), 
            col = "#ffcc00", 
            size = 1.5, 
            lineend = "round", 
            alpha = .6)

clus5 <-  which(clusk10 == 5)

pclus5 <- ggmap(myMapInD) + 
  geom_path(data = data.frame(run_list[[ clus5[1] ]], id = clus5[1]), 
            aes(x = X1, y = X2), 
            col = "#ff1a8c", 
            size = 1.5, 
            lineend = "round", 
            alpha = .6) +
  geom_path(data = data.frame(run_list[[ clus5[2] ]], id = clus5[2]), 
            aes(x = X1, y = X2), 
            col = "#ff6699", 
            size = 1.5, 
            lineend = "round", 
            alpha = .6) +
  geom_path(data = data.frame(run_list[[ clus5[3] ]], id = clus5[3]), 
            aes(x = X1, y = X2), 
            col = "#ffb3d9", 
            size = 1.5, 
            lineend = "round", 
            alpha = .6) 

subplot <- subplot(
  pclus6, pclus1,pclus8, pclus9, pclus5,
  nrows = 3,
  widths = c(0.5, 0.5),
  titleX = TRUE,
  titleY = TRUE,
  shareY = T,
  shareX = T
)

subplot


```

From th plot we an see that Cluster number 6 consists of short runs that start from "the other side" of Tiburtina, go up to Piazza Bologna, then follow the Nomentana and comes back to the starting point, whith some detours in Villa Torlonia. 

Cluster number 1 is characterized by the same route of the orevious one until the Nomentana road, but from that point the run goes on to Villa Borghere (and eventually to Piazza del Popolo), then comes back to the starting point.

Cluster number 8 contains the runs that explore the area on the othere side of the Tevere river, and go near Città del vaticano.

Cluster number 9 has runs that follow a similar path to the ones in cluster n. 1, but they go more on the south near the Tevere.

Cluster number 5 consists of only three runs that are the three plotted above.
